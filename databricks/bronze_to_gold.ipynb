{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b107914-3acb-4e22-9d21-5846501da0cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "#Widgets\n",
    "dbutils.widgets.text(\"storage_account\", \"\", \"Storage account (uten .blob.core)\")\n",
    "dbutils.widgets.text(\"container\", \"\", \"Container\")\n",
    "dbutils.widgets.text(\"account_key\", \"\", \"Account key\")\n",
    "dbutils.widgets.text(\"schema\", \"trafikk\", \"Database/schema\")\n",
    "dbutils.widgets.dropdown(\"overwrite_mode\", \"true\", [\"true\",\"false\"], \"Overwrite Silver/Gold\")\n",
    "\n",
    "dbutils.widgets.text(\"year\", \"\", \"Year (YYYY)\")\n",
    "dbutils.widgets.text(\"month\", \"\", \"Month (MM)\")\n",
    "dbutils.widgets.text(\"day\", \"\", \"Day (DD)\")\n",
    "\n",
    "storage = dbutils.widgets.get(\"storage_account\") or os.environ.get(\"STORAGE_ACCOUNT\", \"\")\n",
    "container = dbutils.widgets.get(\"container\") or os.environ.get(\"CONTAINER_NAME\", \"\")\n",
    "account_key = dbutils.widgets.get(\"account_key\") or os.environ.get(\"ACCOUNT_KEY\", \"\")\n",
    "overwrite = (dbutils.widgets.get(\"overwrite_mode\") == \"true\")\n",
    "schema_db = (dbutils.widgets.get(\"schema\") or \"trafikk\").strip()\n",
    "\n",
    "year  = dbutils.widgets.get(\"year\")\n",
    "month = dbutils.widgets.get(\"month\")\n",
    "day   = dbutils.widgets.get(\"day\")\n",
    "\n",
    "if not (storage and container and account_key):\n",
    "    raise ValueError(\"Mangler storage_account, container eller account_key.\")\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage}.blob.core.windows.net\", account_key)\n",
    "\n",
    "spark.conf.set(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.AzureLogStore\")\n",
    "spark.conf.set(\"fs.azure.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "spark.conf.set(\"fs.AbstractFileSystem.wasb.impl\", \"org.apache.hadoop.fs.azure.Wasb\")\n",
    "spark.conf.set(\"fs.azure.io.retry.max.retries\", \"20\")\n",
    "spark.conf.set(\"fs.azure.io.retry.backoff\", \"5s\")\n",
    "spark.conf.set(\"spark.databricks.delta.retryWriteConflict.enabled\", \"true\")\n",
    "\n",
    "# Base paths\n",
    "bronze_base1 = f\"wasbs://{container}@{storage}.blob.core.windows.net/bronze/traffic\"\n",
    "bronze_base2 = f\"wasbs://{container}@{storage}.blob.core.windows.net/bronze/weather\"\n",
    "silver_base  = f\"wasbs://{container}@{storage}.blob.core.windows.net/silver\"\n",
    "gold_base    = f\"wasbs://{container}@{storage}.blob.core.windows.net/gold\"\n",
    "\n",
    "print(\"Using:\")\n",
    "print(\"  storage :\", storage)\n",
    "print(\"  container :\", container)\n",
    "print(\"  silver :\", silver_base)\n",
    "print(\"  gold :\", gold_base)\n",
    "print(\"  database :\", schema_db)\n",
    "print(\"  overwrite :\", overwrite)\n",
    "print(\"  Y/M/D :\", year, month, day)\n",
    "\n",
    "def _exists(path: str) -> bool:\n",
    "    try:\n",
    "        dbutils.fs.ls(path); return True\n",
    "    except: return False\n",
    "\n",
    "def _is_delta(path: str) -> bool:\n",
    "    return _exists(path + \"/_delta_log\")\n",
    "\n",
    "def register_external_delta(db: str, table: str, path: str, force_replace: bool = False):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\n",
    "    spark.sql(f\"USE {db}\")\n",
    "    if not _is_delta(path):\n",
    "        raise RuntimeError(f\"Stien er ikke en Delta-tabell (mangler _delta_log): {path}\")\n",
    "    if force_replace:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db}.{table}\")\n",
    "        spark.sql(f\"CREATE TABLE {db}.{table} USING DELTA LOCATION '{path}'\")\n",
    "    else:\n",
    "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {db}.{table} USING DELTA LOCATION '{path}'\")\n",
    "\n",
    "def probe_delta_write(base_path: str):\n",
    "    probe = f\"{base_path.rstrip('/')}/_write_probe\"\n",
    "    print(f\"[probe] Writing Delta probe -> {probe}\")\n",
    "    try:\n",
    "        (spark.createDataFrame([(1,)], [\"x\"])\n",
    "              .write.format(\"delta\").mode(\"overwrite\").save(probe))\n",
    "        _ = spark.read.format(\"delta\").load(probe).count()\n",
    "        print(\"[probe] OK\")\n",
    "    except Exception as e:\n",
    "        print(\"[probe] FAIL at:\", probe)\n",
    "        print(e); traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def write_delta_hardened(df, path: str, mode: str, overwrite_schema: bool = True, clean_on_conflict: bool = False):\n",
    "    try:\n",
    "        (df.write.format(\"delta\").mode(mode)\n",
    "           .option(\"overwriteSchema\", \"true\" if overwrite_schema else \"false\")\n",
    "           .save(path))\n",
    "        return\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        conflict = any(s in msg for s in [\n",
    "            \"TASK_WRITE_FAILED\", \"FileAlreadyExists\", \"rename\", \"CommitFailedException\",\n",
    "            \"ConcurrentModificationException\", \"already exists\"\n",
    "        ])\n",
    "        if clean_on_conflict and conflict and mode == \"overwrite\":\n",
    "            print(f\"[write_delta_hardened] Conflict at {path}. Removing path and retrying once...\")\n",
    "            try:\n",
    "                dbutils.fs.rm(path, recurse=True)\n",
    "            except Exception as e2:\n",
    "                print(\"[write_delta_hardened] Failed to remove path:\", e2)\n",
    "            # retry once\n",
    "            (df.write.format(\"delta\").mode(mode)\n",
    "               .option(\"overwriteSchema\", \"true\" if overwrite_schema else \"false\")\n",
    "               .save(path))\n",
    "            return\n",
    "        # If we get here, rethrow\n",
    "        raise\n",
    "\n",
    "# Ensure DB\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_db}\")\n",
    "spark.sql(f\"USE {schema_db}\")\n",
    "print(\"Active DB:\", spark.catalog.currentDatabase())\n",
    "\n",
    "probe_delta_write(silver_base)\n",
    "\n",
    "cand_points = []\n",
    "cand_vols = []\n",
    "cand_rain = []\n",
    "\n",
    "if year and month and day:\n",
    "    cand_points.append(f\"{bronze_base1}/registration_points/{year}/{month}/{day}/flat.parquet\")\n",
    "    cand_vols.append(f\"{bronze_base1}/volumes/{year}/{month}/{day}/flat.parquet\")\n",
    "    cand_rain.append(f\"{bronze_base2}/rain/{year}/{month}/{day}/rain.parquet\")\n",
    "\n",
    "cand_points.append(f\"{bronze_base1}/registration_points/*/*/*/flat.parquet\")\n",
    "cand_vols.append(f\"{bronze_base1}/volumes/*/*/*/flat.parquet\")\n",
    "cand_rain.append(f\"{bronze_base2}/rain/*/*/*/rain.parquet\")\n",
    "\n",
    "def _safe_read_any(globs):\n",
    "    for g in globs:\n",
    "        try:\n",
    "            df = spark.read.parquet(g)\n",
    "            _ = df.limit(1).count()\n",
    "            print(f\"[OK] Loaded {g}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"[MISS] {g} -> {e}\")\n",
    "    return None\n",
    "\n",
    "df_points_bronze = _safe_read_any(cand_points)\n",
    "df_vol_bronze = _safe_read_any(cand_vols)\n",
    "df_rain_bronze = _safe_read_any(cand_rain)\n",
    "\n",
    "if df_points_bronze is None:\n",
    "    raise RuntimeError(\"Fant ingen points-parquet i Bronze (Ã¥r/mnd/dag).\")\n",
    "\n",
    "print(\"Bronze counts -> points:\", df_points_bronze.count(),\n",
    "      \"| volumes:\", (df_vol_bronze.count() if df_vol_bronze else 0),\n",
    "      \"| rain:\", (df_rain_bronze.count() if df_rain_bronze else 0))\n",
    "\n",
    "\n",
    "df_points_clean = (\n",
    "    df_points_bronze\n",
    "    .withColumn(\"id\", F.col(\"id\").cast(StringType()))\n",
    "    .withColumn(\"name\", F.col(\"name\").cast(StringType()))\n",
    "    .withColumn(\"lat\", F.col(\"lat\").cast(DoubleType()))\n",
    "    .withColumn(\"lon\", F.col(\"lon\").cast(DoubleType()))\n",
    "    .withColumn(\"partition_date\", F.to_date(\"partition_date\"))\n",
    "    .dropna(subset=[\"id\",\"lat\",\"lon\"])\n",
    ")\n",
    "try:\n",
    "    w = Window.partitionBy(\"id\").orderBy(F.col(\"partition_date\").desc_nulls_last())\n",
    "except Exception:\n",
    "    w = Window.partitionBy(\"id\").orderBy(F.desc(\"partition_date\"))\n",
    "\n",
    "df_points_silver = (\n",
    "    df_points_clean.withColumn(\"rn\", F.row_number().over(w))\n",
    "                   .filter(\"rn = 1\").drop(\"rn\")\n",
    ")\n",
    "\n",
    "writer_mode = \"overwrite\" if overwrite else \"append\"\n",
    "\n",
    "points_path = f\"{silver_base}/traffic_points\"\n",
    "write_delta_hardened(df_points_silver, points_path, writer_mode, clean_on_conflict=True)\n",
    "register_external_delta(schema_db, \"traffic_points\", points_path, force_replace=True)\n",
    "print(\"Silver points rows:\", spark.read.format(\"delta\").load(points_path).count())\n",
    "\n",
    "# Volumes\n",
    "if df_vol_bronze is not None:\n",
    "    df_vol_silver = (\n",
    "        df_vol_bronze\n",
    "        .withColumn(\"point_id\", F.col(\"point_id\").cast(StringType()))\n",
    "        .withColumn(\"from_ts\", F.to_timestamp(\"from\"))\n",
    "        .withColumn(\"to_ts\", F.to_timestamp(\"to\"))\n",
    "        .withColumn(\"date\", F.to_date(\"from_ts\"))\n",
    "        .withColumn(\"total_volume\", F.col(\"total_volume\").cast(IntegerType()))\n",
    "        .select(\"point_id\",\"date\",\"from_ts\",\"to_ts\",\"total_volume\")\n",
    "        .dropna(subset=[\"point_id\",\"date\"])\n",
    "    )\n",
    "    vols_path = f\"{silver_base}/traffic_volumes\"\n",
    "    write_delta_hardened(df_vol_silver, vols_path, writer_mode, clean_on_conflict=True)\n",
    "    register_external_delta(schema_db, \"traffic_volumes\", vols_path, force_replace=overwrite)\n",
    "else:\n",
    "    spark.sql(f\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS {schema_db}.traffic_volumes\n",
    "      (point_id STRING, date DATE, from_ts TIMESTAMP, to_ts TIMESTAMP, total_volume INT)\n",
    "      USING DELTA LOCATION '{silver_base}/traffic_volumes'\n",
    "    \"\"\")\n",
    "\n",
    "print(\"Silver volumes rows:\",\n",
    "      (spark.read.format(\"delta\").load(f\"{silver_base}/traffic_volumes\").count()\n",
    "       if _is_delta(f\"{silver_base}/traffic_volumes\") else 0))\n",
    "\n",
    "if df_rain_bronze is not None:\n",
    "    df_rain_silver = (\n",
    "        df_rain_bronze\n",
    "        .withColumn(\"point_id\", F.col(\"point_id\").cast(StringType()))\n",
    "        .withColumn(\"date\", F.to_date(\"date\"))\n",
    "        .withColumn(\"precip_mm\", F.col(\"precip_mm\").cast(DoubleType()))\n",
    "        .dropna(subset=[\"point_id\",\"date\"])\n",
    "    )\n",
    "    rain_path = f\"{silver_base}/rain_daily\"\n",
    "    write_delta_hardened(df_rain_silver, rain_path, writer_mode, clean_on_conflict=True)\n",
    "    register_external_delta(schema_db, \"rain_daily\", rain_path, force_replace=overwrite)\n",
    "else:\n",
    "    spark.sql(f\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS {schema_db}.rain_daily\n",
    "      (point_id STRING, date DATE, precip_mm DOUBLE)\n",
    "      USING DELTA LOCATION '{silver_base}/rain_daily'\n",
    "    \"\"\")\n",
    "\n",
    "print(\"Silver rain rows:\",\n",
    "      (spark.read.format(\"delta\").load(f\"{silver_base}/rain_daily\").count()\n",
    "       if _is_delta(f\"{silver_base}/rain_daily\") else 0))\n",
    "\n",
    "# =========================\n",
    "# 4) Gold\n",
    "# =========================\n",
    "sp = spark.read.format(\"delta\").load(points_path)\n",
    "sv = spark.read.format(\"delta\").load(f\"{silver_base}/traffic_volumes\")\n",
    "\n",
    "joined = (\n",
    "    sv.join(sp, sv.point_id == sp.id, \"left\")\n",
    "      .select(sv.point_id, sp.name.alias(\"point_name\"), sp.lat, sp.lon,\n",
    "              sv.date, sv.from_ts, sv.to_ts, sv.total_volume)\n",
    ")\n",
    "\n",
    "gold_daily = (\n",
    "    joined.groupBy(\"point_id\",\"point_name\",\"lat\",\"lon\",\"date\")\n",
    "          .agg(F.sum(\"total_volume\").alias(\"daily_volume\"))\n",
    ")\n",
    "gd_path = f\"{gold_base}/daily_volume_by_point\"\n",
    "write_delta_hardened(gold_daily, gd_path, \"overwrite\", clean_on_conflict=True)\n",
    "register_external_delta(schema_db, \"daily_volume_by_point\", gd_path, force_replace=True)\n",
    "\n",
    "gold_total_daily = joined.groupBy(\"date\").agg(F.sum(\"total_volume\").alias(\"total_daily_volume\"))\n",
    "gtd_path = f\"{gold_base}/total_daily_volume\"\n",
    "write_delta_hardened(gold_total_daily, gtd_path, \"overwrite\", clean_on_conflict=True)\n",
    "register_external_delta(schema_db, \"total_daily_volume\", gtd_path, force_replace=True)\n",
    "\n",
    "sr = (spark.read.format(\"delta\").load(f\"{silver_base}/rain_daily\")\n",
    "      if _is_delta(f\"{silver_base}/rain_daily\")\n",
    "      else spark.createDataFrame([], \"point_id STRING, date DATE, precip_mm DOUBLE\"))\n",
    "\n",
    "gold_volume_vs_rain = (\n",
    "    gold_daily.alias(\"gv\")\n",
    "    .join(sr.alias(\"r\"), [\"point_id\",\"date\"], \"left\")\n",
    "    .select(\"gv.point_id\",\"gv.point_name\",\"gv.lat\",\"gv.lon\",\"gv.date\",\"gv.daily_volume\",\n",
    "            F.col(\"r.precip_mm\").alias(\"precip_mm\"))\n",
    ")\n",
    "gvr_path = f\"{gold_base}/volume_vs_rain_by_point\"\n",
    "write_delta_hardened(gold_volume_vs_rain, gvr_path, \"overwrite\", clean_on_conflict=True)\n",
    "register_external_delta(schema_db, \"volume_vs_rain_by_point\", gvr_path, force_replace=True)\n",
    "\n",
    "display(gold_daily.orderBy(F.col(\"date\").desc(), F.col(\"daily_volume\").desc()))\n",
    "print(\"Gold ready -> tables: daily_volume_by_point, total_daily_volume, volume_vs_rain_by_point\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {schema_db}.dim_date\n",
    "USING DELTA AS\n",
    "SELECT\n",
    "  CAST(date_format(day, 'yyyyMMdd') AS INT) AS date_key,\n",
    "  day AS date,\n",
    "  year(day) AS year,\n",
    "  month(day) AS month,\n",
    "  day(day) AS day_of_month,\n",
    "  weekofyear(day) AS week_of_year,\n",
    "  date_format(day, 'E') AS weekday_name,\n",
    "  CASE WHEN date_format(day, 'E') IN ('Sat','Sun') THEN true ELSE false END AS is_weekend\n",
    "FROM (SELECT explode(sequence(to_date('2025-01-01'), current_date(), INTERVAL 1 DAY)) AS day) s\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {schema_db}.dim_point\n",
    "USING DELTA AS\n",
    "SELECT id AS point_id, name AS point_name, lat, lon\n",
    "FROM {schema_db}.traffic_points\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {schema_db}.fact_traffic_day\n",
    "USING DELTA AS\n",
    "SELECT a.point_id,\n",
    "       CAST(date_format(a.date,'yyyyMMdd') AS INT) AS date_key,\n",
    "       a.date,\n",
    "       a.daily_volume,\n",
    "       COALESCE(r.precip_mm, 0) AS precip_mm\n",
    "FROM {schema_db}.daily_volume_by_point a\n",
    "LEFT JOIN {schema_db}.rain_daily r\n",
    "  ON r.point_id = a.point_id AND r.date = a.date\n",
    "\"\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
